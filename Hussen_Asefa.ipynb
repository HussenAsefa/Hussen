{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d971c7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Modern Information Retrieval(IR) individual Assignment    ###########\n",
      "############     By Hussen Asefa Musa GSE/9375/14     ############################\n"
     ]
    }
   ],
   "source": [
    "print('############ Modern Information Retrieval(IR) individual Assignment    ###########')\n",
    "print('############     By Hussen Asefa Musa GSE/9375/14     ############################')\n",
    "\n",
    "\n",
    "#import all nessary libray's\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7793b6",
   "metadata": {},
   "source": [
    "## 2. Tokenize the files and save the tokinized text as Td1.txt,Td2.txt and Td3.txt respectivelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "95ae931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the files\n",
    "#open and read all three file I crated\n",
    "d1= open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\d1.txt\",encoding=\"utf-8\") \n",
    "d1_lines_list=d1.readlines()\n",
    "d2= open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\d2.txt\",encoding=\"utf-8\") \n",
    "d2_lines_list=d2.readlines()\n",
    "d3= open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\d3.txt\",encoding=\"utf-8\") \n",
    "d3_lines_list=d3.readlines()\n",
    "\n",
    "#create three new files to save toknize file \n",
    "tokenizetext1=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\Td1.txt\", mode=\"w+\", encoding=\"utf-8\")\n",
    "tokenizetext2=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\Td2.txt\", mode=\"w+\", encoding=\"utf-8\")\n",
    "tokenizetext3=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\Td3.txt\", mode=\"w+\", encoding=\"utf-8\")\n",
    "tokenizetext1_line_list = tokenizetext1.readlines()\n",
    "tokenizetext2_line_list = tokenizetext2.readlines()\n",
    "tokenizetext3_line_list = tokenizetext3.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b3c75655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'US', 'assesses', 'Russia', 'possession', 'weapons', 'capable', 'Iranian', 'drones', 'deploy', 'battlefield', 'Ukraine', 'Biden', 'administration', 'officials', 'CNN', 'The', 'Russians', 'picked', 'drones', 'Iranian', 'airfield', 'earlier', 'month', 'transported', 'back', 'Russia', 'cargo', 'planes', 'mid', 'August', 'officials', 'Russian', 'officials', 'began', 'training', 'drones', 'Iran', 'late', 'month', 'CNN', 'previously', 'reported', 'US', 'believes', 'Russia', 'officially', 'purchased', 'transferred', 'Mohajer', 'Shahed', 'series', 'drones', 'Shahed', 'Shahed', 'back', 'Russia', 'Ukraine', 'Both', 'types', 'UAVs', 'unmanned', 'aerial', 'vehicles', 'capable', 'carrying', 'precision', 'guided', 'munitions', 'surveillance', 'Russian', 'operators', 'training', 'drones', 'inside', 'Iran', 'officials', 'US', 'believes', 'Russia', 'intends', 'import', 'hundreds', 'air', 'surface', 'attacks', 'electronic', 'warfare', 'targeting', 'inside', 'Ukraine', 'US', 'intelligence', 'officials', 'believe', 'tested', 'drones', 'Russia', 'purchased', 'Iran', 'experienced', 'numerous', 'failures', 'officials', 'unclear', 'game', 'changer', 'Russians', 'deployed', 'The', 'Washington', 'Post', 'reported', 'drones', 'transferred', 'Russia', 'More', 'background', 'The', 'Biden', 'administration', 'began', 'warning', 'July', 'Russia', 'purchase', 'drones', 'amid', 'acute', 'supply', 'shortages', 'stemming', 'Ukraine', 'Western', 'sanctions', 'stymied', 'production', 'efforts', 'Satellite', 'imagery', 'revealed', 'month', 'showed', 'Russian', 'deegation', 'visited', 'airfield', 'central', 'Iran', 'June', 'exmine', 'weapons', 'capable', 'drones', 'The', 'news', 'drone', 'transfers', 'Biden', 'dministration', 'expressed', 'cautious', 'optimism', 'possible', 'deal', 'revive', 'Iran', 'nuclear', 'deal', 'The', 'deal', 'detractors', 'deal', 'result', 'sanctions', 'relief', 'Iran', 'turn', 'financial', 'windfall', 'enable', 'Iran', 'malign', 'activities', 'region']\n",
      "['Addis', 'Ababa', 'University', 'AAU', 'Office', 'Community', 'Services', 'VP', 'Office', 'Research', 'Technology', 'Transfer', 'organized', 'Green', 'Legacy', 'Campaign', 'University', 'Health', 'Science', 'College', 'Public', 'Health', 'Research', 'Satellite', 'Campus', 'found', 'Butajra', 'town', 'August', 'Abebe', 'Asefa', 'PhD', 'Director', 'Office', 'Community', 'Services', 'AAU', 'officially', 'launched', 'tree', 'planting', 'program', 'expressed', 'gratitude', 'campaign', 'held', 'Satellite', 'Campus', 'number', 'Master', 'PhD', 'scholars', 'University', 'produced', 'According', 'Dr', 'Abebe', 'area', 'planting', 'campaign', 'held', 'property', 'University', 'properly', 'cultiveted', 'great', 'research', 'potential', 'University', 'focus', 'community', 'service', 'educating', 'scientifically', 'shaping', 'local', 'community', 'Dr', 'Abebe', 'stated', 'area', 'walth', 'University', 'vitally', 'important', 'scientifically', 'cultivate', 'valley', 'natura']\n",
      "['forest', 'introducing', 'modern', 'agricultural', 'activities', 'beekeeping', 'future', 'Therefore', 'today', 'brought', 'indigenous', 'trees', 'plant', 'We', 'continue', 'strengthen', 'work', 'replacing', 'trees', 'campus', 'native', 'trees', 'continue', 'work', 'hard', 'preserve', 'beauty', 'campus', 'entire', 'area', 'Dr', 'Abebe', 'finally', 'Coordinator', 'Butajra', 'Satellite', 'Campus', 'Mr', 'Abyot', 'appreciated', 'Community', 'Services', 'Office', 'commitment', 'plant', 'indigenous', 'trees', 'campus', 'traveling', 'Addis', 'promised', 'staff', 'work', 'tirelessly', 'extend', 'startup', 'achieve', 'intended', 'goal', 'Mr', 'Eyob', 'Asfaw', 'Senior', 'Community', 'Service', 'Expert', 'AAU', 'Community', 'Service', 'Office', 'lucky', 'great', 'opportunity', 'putting', 'green', 'footprint', 'Satellite', 'Campus', 'It', 'inevitable', 'campus', 'occasions', 'trees', 'planted', 'grown', 'giving', 'shade', 'proud']\n",
      "['work', 'spiritual', 'satisfaction', 'society', 'benefited', 'activity', 'Eyob', 'elaborated', 'According', 'Mr', 'Eyob', 'indigenous', 'trees', 'including', 'Girar', 'Koso', 'vegetations', 'planted', 'Satellite', 'Campus', 'workers', 'different', 'departments', 'Addis', 'Ababa', 'University', 'working', 'Campus', 'Based', 'information', 'Campus', 'providing', 'public', 'health', 'insights', 'community', 'AAU', 'Butajra', 'Satellite', 'Campus', 'providing', 'sources', 'information', 'publichealth', 'education', 'conducting', 'studies', 'ten', 'districts', 'Butajra', 'town', 'years']\n",
      "['The', 'Addis', 'Ababa', 'City', 'Administration', 'Monday', 'discrepancies', 'distribution', 'Condo', 'units', 'happened', 'week', 'Condominium', 'units', 'distributed', 'latest', 'round', 'lottery', 'draw', 'For', 'residents', 'enter', 'draw', 'supposed', 'registered', 'save', 'required', 'amount', 'money', 'purchase', 'units', 'In', 'statement', 'issued', 'Monday', 'city', 'indicated', 'travelled', 'long', 'distance', 'ensure', 'distribution', 'condo', 'unit', 'fair', 'free', 'It', 'extent', 'leveraging', 'digital', 'technology', 'app', 'development', 'inspected', 'Addis', 'Ababa', 'City', 'Innovation', 'Technology', 'Office', 'Addis', 'Ababa', 'Science', 'Technology', 'office', 'tasked', 'develop', 'software', 'application', 'Ministry', 'Science', 'Technology', 'verify', 'authenticity', 'Professional', 'integrity', 'developers', 'observed', 'city', 'claimed', 'presented', 'professionals', 'unspecified', 'professional', 'selected', 'observers', 'witness', 'authenticity', 'statement', 'Despite', 'city', 'lottery', 'draw', 'distribution', 'Condo', 'units', 'reports', 'apparently', 'dishonest', 'practices', 'process', 'Upon', 'auditing', 'process', 'discrepancy', 'data', 'banks', 'apparently', 'indicating', 'saving', 'condo', 'units', 'requirement', 'lottery', 'draw', 'data', 'uploaded', 'computers', 'Therefore', 'city', 'administration', 'audit', 'established', 'individuals', 'save', 'lottery', 'draw', 'irregularities', 'observed', 'The', 'city', 'suspected', 'executives', 'professionals', 'appeared', 'manufactured', 'irregularity', 'custody', 'It', 'claimed', 'neutral', 'relevant', 'organisations', 'involved', 'investigation', 'underway', 'It', 'asked', 'registered', 'remain', 'patient', 'city', 'announces', 'findings']\n",
      "['investigation', 'And', 'expected', 'happen', 'short', 'time', 'There', 'questions', 'illegal', 'fraudulent', 'condo', 'transfer', 'process', 'city', 'past', 'And', 'looked', 'trajectory', 'ethnic', 'politics', 'determination', 'party', 'radical', 'ethnic', 'politicians', 'power', 'alter', 'demography', 'Addis', 'Ababa', 'There', 'point', 'court', 'suspended', 'distribution', 'Condo', 'units', 'city', 'Adanech', 'Abiebie', 'meant', 'things', 'differently', 'serve', 'residents', 'Addis', 'Ababa']\n",
      "None None None\n"
     ]
    }
   ],
   "source": [
    "#toknize file and save tokneize file into text file\n",
    "def saveToken(file, data_lines_list):\n",
    "    for line in data_lines_list:\n",
    "        sentence = re.sub('[^a-zA-Z]', ' ', line)\n",
    "        toknize_file=word_tokenize(sentence)\n",
    "        #remove stop words\n",
    "        toknize_file_sw = [word for word in toknize_file if not word in stopwords.words()]\n",
    "        print(toknize_file_sw)\n",
    "        file.writelines(str(toknize_file_sw))\n",
    "          \n",
    "tokenizetext1 = saveToken(tokenizetext1,d1_lines_list)\n",
    "tokenizetext2 = saveToken(tokenizetext2,d2_lines_list)\n",
    "tokenizetext3 = saveToken(tokenizetext3,d3_lines_list)\n",
    "print(tokenizetext1,tokenizetext2,tokenizetext3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae7462",
   "metadata": {},
   "source": [
    "## 3. Apply porter or any other stemming algorithm on the extracted tokens and save the stemmed words in a separate file called stemmedtext1.txt, stemmedtext2.txt, stemmedtext3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2150a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the','u','assess','russia','possess','weapon','capabl','iranian','drone','deploy','battlefield','ukrain','biden','administr','offici','cnn','the','russian','pick','drone','iranian','airfield','earlier','month','transport','back','russia','cargo','plane','mid','august','offici','russian','offici','began','train','drone','iran','late','month','cnn','previous','report','u','believ','russia','offici','purchas','transfer','mohaj','shahe','seri','drone','shahe','shahe','back','russia','ukrain','both','type','uav','unman','aerial','vehicl','capabl','carri','precis','guid','munit','surveil','russian','oper','train','drone','insid','iran','offici','u','believ','russia','intend','import','hundr','air','surfac','attack','electron','warfar','target','insid','ukrain','u','intellig','offici','believ','test','drone','russia','purchas','iran','experienc','numer','failur','offici','unclear','game','changer','russian','deploy','the','washington','post','report','drone','transfer','russia','more','background','the','biden','administr','began','warn','juli','russia','purchas','drone','amid','acut','suppli','shortag','stem','ukrain','western','sanction','stymi','product','effort','satellit','imageri','reveal','month','show','russian','deegat','visit','airfield','central','iran','june','exmin','weapon','capabl','drone','the','new','drone','transfer','biden','dministr','express','cautiou','optim','possibl','deal','reviv','iran','nuclear','deal','the','deal','detractor','deal','result','sanction','relief','iran','turn','financi','windfal','enabl','iran','malign','activ','region']\n",
      "['addi','ababa','univers','aau','offic','commun','servic','vp','offic','research','technolog','transfer','organ','green','legaci','campaign','univers','health','scienc','colleg','public','health','research','satellit','campu','found','butajra','town','august','abeb','asefa','phd','director','offic','commun','servic','aau','offici','launch','tree','plant','program','express','gratitud','campaign','held','satellit','campu','number','master','phd','scholar','univers','produc','accord','dr','abeb','area','plant','campaign','held','properti','univers','properli','cultivet','great','research','potenti','univers','focu','commun','servic','educ','scientif','shape','local','commun','dr','abeb','state','area','walth','univers','vital','import','scientif','cultiv','valley','natura']['forest','introduc','modern','agricultur','activ','beekeep','futur','therefor','today','brought','indigen','tree','plant','we','continu','strengthen','work','replac','tree','campu','nativ','tree','continu','work','hard','preserv','beauti','campu','entir','area','dr','abeb','final','coordin','butajra','satellit','campu','mr','abyot','appreci','commun','servic','offic','commit','plant','indigen','tree','campu','travel','addi','promis','staff','work','tirelessli','extend','startup','achiev','intend','goal','mr','eyob','asfaw','senior','commun','servic','expert','aau','commun','servic','offic','lucki','great','opportun','put','green','footprint','satellit','campu','it','inevit','campu','occas','tree','plant','grown','give','shade','proud']['work','spiritu','satisfact','societi','benefit','activ','eyob','elabor','accord','mr','eyob','indigen','tree','includ','girar','koso','veget','plant','satellit','campu','worker','differ','depart','addi','ababa','univers','work','campu','base','inform','campu','provid','public','health','insight','commun','aau','butajra','satellit','campu','provid','sourc','inform','publichealth','educ','conduct','studi','ten','district','butajra','town','year']\n",
      "['the','addi','ababa','citi','administr','monday','discrep','distribut','condo','unit','happen','week','condominium','unit','distribut','latest','round','lotteri','draw','for','resid','enter','draw','suppos','regist','save','requir','amount','money','purchas','unit','in','statement','issu','monday','citi','indic','travel','long','distanc','ensur','distribut','condo','unit','fair','free','it','extent','leverag','digit','technolog','app','develop','inspect','addi','ababa','citi','innov','technolog','offic','addi','ababa','scienc','technolog','offic','task','develop','softwar','applic','ministri','scienc','technolog','verifi','authent','profession','integr','develop','observ','citi','claim','present','profession','unspecifi','profession','select','observ','wit','authent','statement','despit','citi','lotteri','draw','distribut','condo','unit','report','appar','dishonest','practic','process','upon','audit','process','discrep','data','bank','appar','indic','save','condo','unit','requir','lotteri','draw','data','upload','comput','therefor','citi','administr','audit','establish','individu','save','lotteri','draw','irregular','observ','the','citi','suspect','execut','profession','appear','manufactur','irregular','custodi','it','claim','neutral','relev','organis','involv','investig','underway','it','ask','regist','remain','patient','citi','announc','find']['investig','and','expect','happen','short','time','there','question','illeg','fraudul','condo','transfer','process','citi','past','and','look','trajectori','ethnic','polit','determin','parti','radic','ethnic','politician','power','alter','demographi','addi','ababa','there','point','court','suspend','distribut','condo','unit','citi','adanech','abiebi','meant','thing','differ','serv','resid','addi','ababa']\n",
      "[\"['The', 'Addis', 'Ababa', 'City', 'Administration', 'Monday', 'discrepancies', 'distribution', 'Condo', 'units', 'happened', 'week', 'Condominium', 'units', 'distributed', 'latest', 'round', 'lottery', 'draw', 'For', 'residents', 'enter', 'draw', 'supposed', 'registered', 'save', 'required', 'amount', 'money', 'purchase', 'units', 'In', 'statement', 'issued', 'Monday', 'city', 'indicated', 'travelled', 'long', 'distance', 'ensure', 'distribution', 'condo', 'unit', 'fair', 'free', 'It', 'extent', 'leveraging', 'digital', 'technology', 'app', 'development', 'inspected', 'Addis', 'Ababa', 'City', 'Innovation', 'Technology', 'Office', 'Addis', 'Ababa', 'Science', 'Technology', 'office', 'tasked', 'develop', 'software', 'application', 'Ministry', 'Science', 'Technology', 'verify', 'authenticity', 'Professional', 'integrity', 'developers', 'observed', 'city', 'claimed', 'presented', 'professionals', 'unspecified', 'professional', 'selected', 'observers', 'witness', 'authenticity', 'statement', 'Despite', 'city', 'lottery', 'draw', 'distribution', 'Condo', 'units', 'reports', 'apparently', 'dishonest', 'practices', 'process', 'Upon', 'auditing', 'process', 'discrepancy', 'data', 'banks', 'apparently', 'indicating', 'saving', 'condo', 'units', 'requirement', 'lottery', 'draw', 'data', 'uploaded', 'computers', 'Therefore', 'city', 'administration', 'audit', 'established', 'individuals', 'save', 'lottery', 'draw', 'irregularities', 'observed', 'The', 'city', 'suspected', 'executives', 'professionals', 'appeared', 'manufactured', 'irregularity', 'custody', 'It', 'claimed', 'neutral', 'relevant', 'organisations', 'involved', 'investigation', 'underway', 'It', 'asked', 'registered', 'remain', 'patient', 'city', 'announces', 'findings']['investigation', 'And', 'expected', 'happen', 'short', 'time', 'There', 'questions', 'illegal', 'fraudulent', 'condo', 'transfer', 'process', 'city', 'past', 'And', 'looked', 'trajectory', 'ethnic', 'politics', 'determination', 'party', 'radical', 'ethnic', 'politicians', 'power', 'alter', 'demography', 'Addis', 'Ababa', 'There', 'point', 'court', 'suspended', 'distribution', 'Condo', 'units', 'city', 'Adanech', 'Abiebie', 'meant', 'things', 'differently', 'serve', 'residents', 'Addis', 'Ababa']\"]\n"
     ]
    }
   ],
   "source": [
    "# steaming function using porter steammer using nltk library\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\"\")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "#call tokinized file\n",
    "tokenizetext1=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\Td1.txt\",encoding=\"utf-8\")\n",
    "tokenizetext2=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\Td2.txt\",encoding=\"utf-8\")\n",
    "tokenizetext3=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\Td3.txt\",encoding=\"utf-8\")\n",
    "tokenizetext1_line_list = tokenizetext1.readlines()\n",
    "tokenizetext2_line_list = tokenizetext2.readlines()\n",
    "tokenizetext3_line_list = tokenizetext3.readlines()\n",
    "\n",
    "#create the new files\n",
    "stemmedtext1=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\stemmedtext1.txt\", mode=\"w\", encoding=\"utf-8\")\n",
    "stemmedtext2=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\stemmedtext2.txt\", mode=\"w\", encoding=\"utf-8\")\n",
    "stemmedtext3=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\stemmedtext3.txt\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "#save steamed file into text file\n",
    "def saveSteam(file, data_lines_list):\n",
    "    for line in data_lines_list:\n",
    "        stem_sentence=stemSentence(line)\n",
    "        print(stem_sentence)\n",
    "        file.write(stem_sentence)\n",
    "\n",
    "saveSteam(stemmedtext1,tokenizetext1_line_list)\n",
    "saveSteam(stemmedtext2,tokenizetext2_line_list)\n",
    "saveSteam(stemmedtext3,tokenizetext3_line_list)\n",
    "print(tokenizetext3_line_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f1357f",
   "metadata": {},
   "source": [
    "##  4.Extract each stemmed token and their frequency of occurrence for the stemmed files in a separate file named freq1.txt, freq2.txt, freq3.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59e09f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[ addi ': 1, ' ababa ': 2, ' univers ': 7, ' aau ': 4, ' offic ': 5, ' commun ': 8, ' servic ': 6, ' vp ': 1, ' research ': 3, ' technolog ': 1, ' transfer ': 1, ' organ ': 1, ' green ': 2, ' legaci ': 1, ' campaign ': 3, ' health ': 3, ' scienc ': 1, ' colleg ': 1, ' public ': 2, ' satellit ': 6, ' campu ': 12, ' found ': 1, ' butajra ': 4, ' town ': 2, ' august ': 1, ' abeb ': 4, ' asefa ': 1, ' phd ': 2, ' director ': 1, ' offici ': 1, ' launch ': 1, ' tree ': 7, ' plant ': 6, ' program ': 1, ' express ': 1, ' gratitud ': 1, ' held ': 2, ' number ': 1, ' master ': 1, ' scholar ': 1, ' produc ': 1, ' accord ': 2, ' dr ': 3, ' area ': 3, ' properti ': 1, ' properli ': 1, ' cultivet ': 1, ' great ': 2, ' potenti ': 1, ' focu ': 1, ' educ ': 2, ' scientif ': 2, ' shape ': 1, ' local ': 1, ' state ': 1, ' walth ': 1, ' vital ': 1, ' import ': 1, ' cultiv ': 1, ' valley ': 1, ' natura ][ forest ': 1, ' introduc ': 1, ' modern ': 1, ' agricultur ': 1, ' activ ': 2, ' beekeep ': 1, ' futur ': 1, ' therefor ': 1, ' today ': 1, ' brought ': 1, ' indigen ': 3, ' we ': 1, ' continu ': 2, ' strengthen ': 1, ' work ': 4, ' replac ': 1, ' nativ ': 1, ' hard ': 1, ' preserv ': 1, ' beauti ': 1, ' entir ': 1, ' final ': 1, ' coordin ': 1, ' mr ': 3, ' abyot ': 1, ' appreci ': 1, ' commit ': 1, ' travel ': 1, ' addi ': 2, ' promis ': 1, ' staff ': 1, ' tirelessli ': 1, ' extend ': 1, ' startup ': 1, ' achiev ': 1, ' intend ': 1, ' goal ': 1, ' eyob ': 3, ' asfaw ': 1, ' senior ': 1, ' expert ': 1, ' lucki ': 1, ' opportun ': 1, ' put ': 1, ' footprint ': 1, ' it ': 1, ' inevit ': 1, ' occas ': 1, ' grown ': 1, ' give ': 1, ' shade ': 1, ' proud ][ work ': 1, ' spiritu ': 1, ' satisfact ': 1, ' societi ': 1, ' benefit ': 1, ' elabor ': 1, ' includ ': 1, ' girar ': 1, ' koso ': 1, ' veget ': 1, ' worker ': 1, ' differ ': 1, ' depart ': 1, ' base ': 1, ' inform ': 2, ' provid ': 2, ' insight ': 1, ' sourc ': 1, ' publichealth ': 1, ' conduct ': 1, ' studi ': 1, ' ten ': 1, ' district ': 1, ' year ]': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1896"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create three files to save frequency of occurrence of stemmed tokens\n",
    "stemmedtext1=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\stemmedtext1.txt\",encoding=\"utf-8\")\n",
    "stemmedtext2=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\stemmedtext2.txt\",encoding=\"utf-8\")\n",
    "stemmedtext3=open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\stemmedtext3.txt\",encoding=\"utf-8\")\n",
    "stemmedtext1_line_list = stemmedtext1.read()\n",
    "stemmedtext2_line_list = stemmedtext2.read()\n",
    "stemmedtext3_line_list = stemmedtext3.read()\n",
    "stemmedtext3_line_list = re.sub(\"'\", ' ', stemmedtext3_line_list)\n",
    "stemmedtext2_line_list = re.sub(\"'\", ' ', stemmedtext2_line_list)\n",
    "stemmedtext1_line_list = re.sub(\"'\", ' ', stemmedtext1_line_list)\n",
    "\n",
    "#print(stemmedtext1_line_list.split())\n",
    "#a function that calculate frequency of stemmed tokens \n",
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(list(zip(wordlist,wordfreq)))\n",
    "#for example print word frequence of tokens in stemmedtext1\n",
    "print(wordListToFreqDict(stemmedtext2_line_list.split(\",\")))\n",
    "\n",
    "file1 = open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\freq1.txt\",'w',encoding='utf-8')\n",
    "file1.write(str(wordListToFreqDict(stemmedtext1_line_list.split(\",\"))))\n",
    "file2 = open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\freq2.txt\",'w',encoding='utf-8')\n",
    "file2.write(str(wordListToFreqDict(stemmedtext2_line_list.split(\",\"))))\n",
    "file3 = open(\"C:\\\\Users\\\\User\\\\Desktop\\\\2014 summer course\\\\Assignment\\\\text folder\\\\freq3.txt\",'w',encoding='utf-8')\n",
    "file3.write(str(wordListToFreqDict(stemmedtext3_line_list.split(\",\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c27ea6",
   "metadata": {},
   "source": [
    "## 5.Finally collect each term in a term document matrix like structure and compute the necessary weights (where weight is the frequency of the terms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ee563aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      d1        d2        d3\n",
      "129             0.031667  0.000000  0.000000\n",
      "16th            0.000000  0.023332  0.000000\n",
      "191             0.031667  0.000000  0.000000\n",
      "2022            0.000000  0.046665  0.000000\n",
      "25              0.000000  0.000000  0.024092\n",
      "30              0.000000  0.023332  0.000000\n",
      "400             0.000000  0.023332  0.000000\n",
      "491             0.000000  0.000000  0.024092\n",
      "aau             0.000000  0.093329  0.000000\n",
      "ababa           0.000000  0.035490  0.091613\n",
      "abebe           0.000000  0.093329  0.000000\n",
      "abiebie         0.000000  0.000000  0.024092\n",
      "about           0.018703  0.013780  0.042687\n",
      "abyot           0.000000  0.023332  0.000000\n",
      "according       0.000000  0.046665  0.000000\n",
      "achieve         0.000000  0.023332  0.000000\n",
      "activities      0.024083  0.017745  0.000000\n",
      "activity        0.000000  0.023332  0.000000\n",
      "acute           0.031667  0.000000  0.000000\n",
      "adanech         0.000000  0.000000  0.024092\n",
      "addis           0.000000  0.053235  0.091613\n",
      "administration  0.048167  0.000000  0.036645\n",
      "aerial          0.031667  0.000000  0.000000\n",
      "after           0.000000  0.000000  0.024092\n",
      "agricultural    0.000000  0.023332  0.000000\n",
      "air             0.031667  0.000000  0.000000\n",
      "airfield        0.063334  0.000000  0.000000\n",
      "all             0.000000  0.000000  0.024092\n",
      "already         0.031667  0.000000  0.000000\n",
      "also            0.000000  0.017745  0.073290\n"
     ]
    }
   ],
   "source": [
    "# create term document matrix\n",
    "df1 = pd.DataFrame({'d1': [str(d1_lines_list)], 'd2': [str(d2_lines_list)], 'd3': [str(d3_lines_list)]})\n",
    "# Initialize\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vec = vectorizer.fit_transform(df1.iloc[0])\n",
    "# Create dataFrame\n",
    "df2 = pd.DataFrame(doc_vec.toarray().transpose(),\n",
    "                   index=vectorizer.get_feature_names())\n",
    " \n",
    "# Change column headers\n",
    "df2.columns = df1.columns\n",
    "print(df2.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33b541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
